{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Question A2 (10 marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In this question, we will determine the optimal batch size for mini-batch gradient descent. Find the optimal batch size for mini-batch gradient descent by training the neural network and evaluating the performances for different batch sizes. Note: Use 5-fold cross-validation on the training partition to perform hyperparameter selection. You will have to reconsider the scaling of the dataset during the 5-fold cross validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot mean cross-validation accuracies on the final epoch for different batch sizes as a scatter plot. Limit search space to batch sizes {64, 128, 256, 512}. Next, create a table of time taken to train the network on the last epoch against different batch sizes. Finally, select the optimal batch size and state a reason for your selection.\n",
    "\n",
    "\n",
    "This might take a while to run, so plan your time carefully."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Admin\\anaconda3\\envs\\myenv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from scipy.io import wavfile as wav\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from common_utils import set_seed\n",
    "\n",
    "# setting seed\n",
    "set_seed()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1.To reduce repeated code, place your\n",
    "\n",
    "- network (MLP defined in QA1)\n",
    "\n",
    "- torch datasets (CustomDataset defined in QA1)\n",
    "- loss function (loss_fn defined in QA1)\n",
    "in a separate file called common_utils.py\n",
    "\n",
    "Import them into this file. You will not be repenalised for any error in QA1 here as the code in QA1 will not be remarked.\n",
    "\n",
    "The following code cell will not be marked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from common_utils import MLP, CustomDataset, EarlyStopper, preprocess_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2.Define different folds for different batch sizes to get a dictionary of training and validation datasets. Preprocess your datasets accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_features = 77\n",
    "no_hidden = 128\n",
    "no_labels = 1\n",
    "\n",
    "no_folds = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_cv_folds_for_batch_sizes(parameters, X_train, y_train):\n",
    "    \"\"\"\n",
    "    returns:\n",
    "    X_train_scaled_dict(dict) where X_train_scaled_dict[batch_size] is a list of the preprocessed training matrix for the different folds.\n",
    "    X_val_scaled_dict(dict) where X_val_scaled_dict[batch_size] is a list of the processed validation matrix for the different folds.\n",
    "    y_train_dict(dict) where y_train_dict[batch_size] is a list of labels for the different folds\n",
    "    y_val_dict(dict) where y_val_dict[batch_size] is a list of labels for the different folds\n",
    "    \"\"\"\n",
    "    # create KFold object\n",
    "    kf = KFold(n_splits=no_folds, shuffle=True, random_state=42)\n",
    "\n",
    "    # create dicts\n",
    "    X_train_scaled_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "    X_val_scaled_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "    y_train_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "    y_val_dict = {batch_size: [] for batch_size in batch_sizes}\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_train_fold, X_val_fold = X_train[train_index], X_train[val_index]\n",
    "        y_train_fold, y_val_fold = y_train[train_index], y_train[val_index]\n",
    "        \n",
    "        # Scaling\n",
    "        X_train_scaled, X_val_scaled = preprocess_dataset(X_train_fold, X_val_fold)\n",
    "        \n",
    "        for batch_size in batch_sizes:\n",
    "            X_train_scaled_dict[batch_size].append(X_train_scaled)\n",
    "            X_val_scaled_dict[batch_size].append(X_val_scaled)\n",
    "            y_train_dict[batch_size].append(y_train_fold)\n",
    "            y_val_dict[batch_size].append(y_val_fold)\n",
    "    \n",
    "    return X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict\n",
    "\n",
    "df = pd.read_csv('simplified.csv')\n",
    "df['label'] = df['filename'].str.split('_').str[-2]\n",
    "\n",
    "df['label'].value_counts()\n",
    "\n",
    "\n",
    "# Remove 'filename' and 'label' columns from features\n",
    "X_train = df.drop(['filename', 'label'], axis=1)\n",
    "y_train = preprocessing.LabelEncoder().fit_transform(df['label'])\n",
    "\n",
    "batch_sizes = [64, 128, 256, 512]\n",
    "X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict = generate_cv_folds_for_batch_sizes(batch_sizes, X_train.to_numpy(), y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.Perform hyperparameter tuning for the different batch sizes with 5-fold cross validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes, parameters):\n",
    "    cross_validation_accuracies = {}\n",
    "    cross_validation_times = {}\n",
    "    last_epoch_times = {}\n",
    "\n",
    "    for batch_size in batch_sizes:\n",
    "        fold_accuracies = []\n",
    "        fold_times = []\n",
    "        fold_last_epoch_times = []\n",
    "\n",
    "        for fold in range(len(X_train_scaled_dict[batch_size])):\n",
    "            start_time = time.time()\n",
    "\n",
    "            # Dataset & Dataloader objects\n",
    "            train_dataset = CustomDataset(X_train_scaled_dict[batch_size][fold], y_train_dict[batch_size][fold])\n",
    "            test_dataset = CustomDataset(X_val_scaled_dict[batch_size][fold], y_val_dict[batch_size][fold])\n",
    "            train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "            test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "            # Initialize model, loss function, and optimizer\n",
    "            model = MLP(X_train.shape[1], 128, 1)\n",
    "            loss_fn = nn.BCELoss()\n",
    "            optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "            # Training loop\n",
    "            n_epochs = 15\n",
    "            for epoch in range(n_epochs):\n",
    "                model.train()\n",
    "                if epoch == n_epochs - 1:  # If it's the last epoch\n",
    "                    last_epoch_start_time = time.time()  # Start timing the last epoch\n",
    "                    \n",
    "                for X_batch, y_batch in train_loader:\n",
    "                    y_batch = y_batch.float().unsqueeze(1)\n",
    "                    y_pred = model(X_batch)\n",
    "                    loss = loss_fn(y_pred, y_batch)\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                if epoch == n_epochs - 1:  # If it's the last epoch\n",
    "                    last_epoch_time = time.time() - last_epoch_start_time  # Calculate time for the last epoch\n",
    "                    fold_last_epoch_times.append(last_epoch_time)  # Add to the list\n",
    "\n",
    "            # Evaluation\n",
    "            model.eval()\n",
    "            with torch.no_grad():\n",
    "                y_pred = model(test_dataset.X)\n",
    "                accuracy = ((y_pred > 0.5).float() == test_dataset.y.unsqueeze(1).float()).float().mean().item()\n",
    "\n",
    "            fold_accuracies.append(accuracy)\n",
    "            fold_times.append(time.time() - start_time)\n",
    "\n",
    "        print(fold_accuracies)\n",
    "        print(fold_times)\n",
    "        \n",
    "        cross_validation_accuracies[batch_size] = mean(fold_accuracies)\n",
    "        cross_validation_times[batch_size] = mean(fold_times)\n",
    "        last_epoch_times[batch_size] = fold_last_epoch_times\n",
    "\n",
    "    return cross_validation_accuracies, cross_validation_times, last_epoch_times\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.746268630027771, 0.7524875402450562, 0.7494815587997437, 0.7490667700767517, 0.7349647283554077]\n",
      "[3.738298177719116, 3.740811347961426, 3.7538812160491943, 3.8152055740356445, 3.769463539123535]\n",
      "[0.7446103096008301, 0.7508291602134705, 0.7320613861083984, 0.7445043325424194, 0.7482372522354126]\n",
      "[2.8103840351104736, 2.8134000301361084, 2.804352283477783, 2.7741920948028564, 2.760620594024658]\n",
      "[0.7330016493797302, 0.7251243591308594, 0.726669430732727, 0.7196184396743774, 0.7216922640800476]\n",
      "[2.3228020668029785, 2.362009286880493, 2.2619800567626953, 2.3283307552337646, 2.3424062728881836]\n",
      "[0.7180762887001038, 0.7160032987594604, 0.711323082447052, 0.7055163979530334, 0.7080049514770508]\n",
      "[1.9091100692749023, 1.916651725769043, 2.014669179916382, 1.9040842056274414, 1.9282119274139404]\n"
     ]
    }
   ],
   "source": [
    "batch_sizes = [64, 128, 256, 512]\n",
    "cross_validation_accuracies, cross_validation_times, last_epoch_times = find_optimal_hyperparameter(X_train_scaled_dict, X_val_scaled_dict, y_train_dict, y_val_dict, batch_sizes, 'batch_size')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch size: 64\n",
      "  Average accuracy: 0.7465\n",
      "  Average time: 3.76 seconds\n",
      "Batch size: 128\n",
      "  Average accuracy: 0.7440\n",
      "  Average time: 2.79 seconds\n",
      "Batch size: 256\n",
      "  Average accuracy: 0.7252\n",
      "  Average time: 2.32 seconds\n",
      "Batch size: 512\n",
      "  Average accuracy: 0.7118\n",
      "  Average time: 1.93 seconds\n"
     ]
    }
   ],
   "source": [
    "# Print results\n",
    "mean_accuracies = []\n",
    "\n",
    "for batch_size in batch_sizes:\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    print(f\"  Average accuracy: {cross_validation_accuracies[batch_size]:.4f}\")\n",
    "    mean_accuracies.append(cross_validation_accuracies[batch_size])\n",
    "    print(f\"  Average time: {cross_validation_times[batch_size]:.2f} seconds\")\n",
    "\n",
    "# Find best batch size\n",
    "# best_batch_size = max(cross_validation_accuracies, key=cross_validation_accuracies.get)\n",
    "# print(f\"\\nBest batch size: {best_batch_size}\")\n",
    "# print(f\"Best average accuracy: {cross_validation_accuracies[best_batch_size]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Plot scatterplot of mean cross validation accuracies for the different batch sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x576 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(figsize=(10,8))\n",
    "plt.plot(batch_sizes, mean_accuracies, marker = 'x', linestyle = 'None')\n",
    "plt.xticks(batch_sizes)\n",
    "plt.xlabel('batch sizes')\n",
    "plt.ylabel('mean cross validation accuracies')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4.Create a table of time taken to train the network on the last epoch against different batch sizes. Select the optimal batch size and state a reason for your selection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Batch Size</th>\n",
       "      <th>Fold number</th>\n",
       "      <th>Last Epoch Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0.247309</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>64</td>\n",
       "      <td>2</td>\n",
       "      <td>0.245299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>64</td>\n",
       "      <td>3</td>\n",
       "      <td>0.252337</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>64</td>\n",
       "      <td>4</td>\n",
       "      <td>0.249823</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>64</td>\n",
       "      <td>5</td>\n",
       "      <td>0.251331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>128</td>\n",
       "      <td>1</td>\n",
       "      <td>0.186488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>128</td>\n",
       "      <td>2</td>\n",
       "      <td>0.185482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>128</td>\n",
       "      <td>3</td>\n",
       "      <td>0.183975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>128</td>\n",
       "      <td>4</td>\n",
       "      <td>0.181963</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>128</td>\n",
       "      <td>5</td>\n",
       "      <td>0.183472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>256</td>\n",
       "      <td>1</td>\n",
       "      <td>0.147280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>256</td>\n",
       "      <td>2</td>\n",
       "      <td>0.152307</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>256</td>\n",
       "      <td>3</td>\n",
       "      <td>0.145270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>256</td>\n",
       "      <td>4</td>\n",
       "      <td>0.141751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>256</td>\n",
       "      <td>5</td>\n",
       "      <td>0.155826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>512</td>\n",
       "      <td>1</td>\n",
       "      <td>0.112093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>512</td>\n",
       "      <td>2</td>\n",
       "      <td>0.125666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>512</td>\n",
       "      <td>3</td>\n",
       "      <td>0.155825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>512</td>\n",
       "      <td>4</td>\n",
       "      <td>0.126671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>512</td>\n",
       "      <td>5</td>\n",
       "      <td>0.126671</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Batch Size  Fold number  Last Epoch Time\n",
       "0           64            1         0.247309\n",
       "1           64            2         0.245299\n",
       "2           64            3         0.252337\n",
       "3           64            4         0.249823\n",
       "4           64            5         0.251331\n",
       "5          128            1         0.186488\n",
       "6          128            2         0.185482\n",
       "7          128            3         0.183975\n",
       "8          128            4         0.181963\n",
       "9          128            5         0.183472\n",
       "10         256            1         0.147280\n",
       "11         256            2         0.152307\n",
       "12         256            3         0.145270\n",
       "13         256            4         0.141751\n",
       "14         256            5         0.155826\n",
       "15         512            1         0.112093\n",
       "16         512            2         0.125666\n",
       "17         512            3         0.155825\n",
       "18         512            4         0.126671\n",
       "19         512            5         0.126671"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create lists to hold the data for the DataFrame\n",
    "batch_size_list = []\n",
    "last_epoch_time_list = []\n",
    "fold_list = [1,2,3,4,5,1,2,3,4,5,1,2,3,4,5,1,2,3,4,5]\n",
    "\n",
    "# Populate the lists\n",
    "for batch_size in batch_sizes:\n",
    "    for time in last_epoch_times[batch_size]:\n",
    "        batch_size_list.append(batch_size)\n",
    "        last_epoch_time_list.append(time)\n",
    "\n",
    "# Create the DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Batch Size': batch_size_list,\n",
    "    'Fold number': fold_list,\n",
    "    'Last Epoch Time': last_epoch_time_list\n",
    "})\n",
    "\n",
    "# Print DataFrame\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            count      mean       std       min       25%       50%       75%  \\\n",
      "Batch Size                                                                      \n",
      "64            5.0  0.249220  0.002896  0.245299  0.247309  0.249823  0.251331   \n",
      "128           5.0  0.184276  0.001763  0.181963  0.183472  0.183975  0.185482   \n",
      "256           5.0  0.148487  0.005604  0.141751  0.145270  0.147280  0.152307   \n",
      "512           5.0  0.129385  0.016021  0.112093  0.125666  0.126671  0.126671   \n",
      "\n",
      "                 max  \n",
      "Batch Size            \n",
      "64          0.252337  \n",
      "128         0.186488  \n",
      "256         0.155826  \n",
      "512         0.155825  \n"
     ]
    }
   ],
   "source": [
    "# Checking mean last epoch time against batch sizes\n",
    "print(df.groupby('Batch Size')['Last Epoch Time'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "optimal_batch_size = 64\n",
    "reason = \"Batch size of 64 has the highest cross-validation accuracy, even though it has the longest average last epoch training time.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cnnkernel",
   "language": "python",
   "name": "cnnkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
